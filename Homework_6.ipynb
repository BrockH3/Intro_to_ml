{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFaOR/mGJcQEMr+Mu3yUMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrockH3/Intro_to_ml/blob/main/Homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XedDYHxiUR7O",
        "outputId": "5917d512-03e2-4956-8016-e7c9a5ebc19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "url='/content/drive/My Drive/Intro to ML/datasets/Housing.csv'\n",
        "housing=pd.DataFrame(pd.read_csv(url))\n",
        "\n",
        "# set up housing_1 for problem 2 and housing for problem 3\n",
        "variables =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "#defining map function to change yes to 1 and no to 0\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0,})\n",
        "\n",
        "housing[variables] = housing[variables].apply(binary_map)         #applying it to variable list\n",
        "housing = housing.drop(columns = 'furnishingstatus')              #housing holds all values in csv file\n",
        "\n",
        "#scale all inputs to be between 1 and 0\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\n",
        "\n",
        "housing[num_vars] = scaler.fit_transform(housing[num_vars])\n",
        "\n",
        "\n",
        "housing_c = torch.from_numpy(housing.pop('price').to_numpy())\n",
        "housing_u = torch.from_numpy(housing.to_numpy())\n"
      ],
      "metadata": {
        "id": "yhCNT7qCUbse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomize training set and validation set for housing set\n",
        "\n",
        "n = housing_u.shape[0]\n",
        "n_val = int(0.2 * n)\n",
        "\n",
        "shuffled = torch.randperm(n)      # randomizes number of samples\n",
        "\n",
        "train_set = shuffled[:-n_val]     #selects samples to be in training and validation set\n",
        "val_set = shuffled[-n_val:]\n",
        "\n",
        "#set training input and output\n",
        "train_t_u = housing_u[train_set]\n",
        "train_t_c = housing_c[train_set]\n",
        "val_t_u = housing_u[val_set]\n",
        "val_t_c = housing_c[val_set]\n"
      ],
      "metadata": {
        "id": "ZgOSAwGYjKBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(epochs, optimizer, model, loss_function, train_t_u, train_t_c, val_t_u, val_t_c):\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train_t_p = model(train_t_u)\n",
        "      train_loss = loss_function(train_t_p, train_t_c)\n",
        "\n",
        "      val_t_p = model(val_t_u)\n",
        "      val_loss = loss_function(val_t_p, val_t_c)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (epoch % 500 == 0):\n",
        "        print('epoch: ', epoch, ' training loss: ', float(train_loss), ' validation loss: ', float(val_loss))"
      ],
      "metadata": {
        "id": "m8xbTVEvYn9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(11,32), nn.Tanh(),nn.Linear(32,1))\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
        "\n",
        "training_loop(epochs = 5000,\n",
        "              optimizer = optimizer,\n",
        "              model = model.double(),\n",
        "              loss_function = nn.MSELoss(),\n",
        "              train_t_u = train_t_u,\n",
        "              train_t_c = train_t_c,\n",
        "              val_t_u = val_t_u,\n",
        "              val_t_c = val_t_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2EYQyN5UkW-",
        "outputId": "838a560f-1505-48c6-cddd-383b8861a6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  500  training loss:  0.02882449634274625  validation loss:  0.031141956927898812\n",
            "epoch:  1000  training loss:  0.028082332982287746  validation loss:  0.030393849825273482\n",
            "epoch:  1500  training loss:  0.02759139178074202  validation loss:  0.029912212583294736\n",
            "epoch:  2000  training loss:  0.02725161414828161  validation loss:  0.02958116397685634\n",
            "epoch:  2500  training loss:  0.027006077186363043  validation loss:  0.029343159796171428\n",
            "epoch:  3000  training loss:  0.026821573890826178  validation loss:  0.029164823972197846\n",
            "epoch:  3500  training loss:  0.026678210408275567  validation loss:  0.02902621389493633\n",
            "epoch:  4000  training loss:  0.026563702994406503  validation loss:  0.028915085887250093\n",
            "epoch:  4500  training loss:  0.026470207233875612  validation loss:  0.028823694339449354\n",
            "epoch:  5000  training loss:  0.026392529958111373  validation loss:  0.02874697759776103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(11,32),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(32,64),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(64,16),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(16,1))\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
        "\n",
        "training_loop(epochs = 5000,\n",
        "              optimizer = optimizer,\n",
        "              model = model.double(),\n",
        "              loss_function = nn.MSELoss(),\n",
        "              train_t_u = train_t_u,\n",
        "              train_t_c = train_t_c,\n",
        "              val_t_u = val_t_u,\n",
        "              val_t_c = val_t_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjkRx-D41Prx",
        "outputId": "4b44cd12-9b7a-43a2-ebf5-91f1ad4376d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  500  training loss:  0.027024870705850843  validation loss:  0.030122227378176814\n",
            "epoch:  1000  training loss:  0.02638650765083517  validation loss:  0.028557364159040942\n",
            "epoch:  1500  training loss:  0.026345901968859966  validation loss:  0.028469144282942675\n",
            "epoch:  2000  training loss:  0.026310032086362456  validation loss:  0.028427940254276394\n",
            "epoch:  2500  training loss:  0.026277178233680115  validation loss:  0.028391890348315393\n",
            "epoch:  3000  training loss:  0.0262470392124458  validation loss:  0.02835876619586319\n",
            "epoch:  3500  training loss:  0.026219351147648073  validation loss:  0.028328207305929606\n",
            "epoch:  4000  training loss:  0.026193879785768297  validation loss:  0.0282999711585475\n",
            "epoch:  4500  training loss:  0.026170416646924842  validation loss:  0.028273845561430297\n",
            "epoch:  5000  training loss:  0.02614877575766326  validation loss:  0.028249641289654317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "datapath = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(datapath, train = True, download = True, transform = transforms.ToTensor())\n",
        "cifar10val = datasets.CIFAR10(datapath, train = False, download = True, transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10,batch_size = 64, shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10val,batch_size = 64, shuffle = True)\n"
      ],
      "metadata": {
        "id": "R9YIUwHcgHph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109f15db-cb93-49a7-b76f-ef145754464f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 84753704.79it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(3072,512),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(512,10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr = .01)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    output = model(imgs.view(batch_size,-1))\n",
        "    loss = loss_function(output,labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if (epoch%50 == 0):\n",
        "    print('epoch: ', epoch, ' loss: ', float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4jtjOwSg_DK",
        "outputId": "937caa5a-adeb-4c64-b1f1-fed576dea632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  1.82626211643219\n",
            "epoch:  50  loss:  1.2656434774398804\n",
            "epoch:  100  loss:  1.579761028289795\n",
            "epoch:  150  loss:  0.5713603496551514\n",
            "epoch:  200  loss:  0.6645681858062744\n",
            "epoch:  250  loss:  0.39324361085891724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for img, labels in val_loader:\n",
        "    batch_size = img.shape[0]\n",
        "    output = model(img.view(batch_size,-1))\n",
        "    _, predicted = torch.max(output,dim=1)\n",
        "    total += labels.shape[0]\n",
        "    correct+=int((predicted==labels).sum())\n",
        "print('accuracy: ', correct/total)\n"
      ],
      "metadata": {
        "id": "Ur_fxxympM74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd93a3d-aa23-4866-8bfb-4dbd8502d0d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.0921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(3072,512),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(512,256),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(256,128),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(128,10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr = .01)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    output = model(imgs.view(batch_size,-1))\n",
        "    loss = loss_function(output,labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if (epoch%50 == 0):\n",
        "    print('epoch: ', epoch, ' loss: ', float(loss))"
      ],
      "metadata": {
        "id": "Jr3UgjbFJduW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c489e58b-12c7-4488-b1d9-3b12efd64c41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  1.7726235389709473\n",
            "epoch:  50  loss:  1.2572051286697388\n",
            "epoch:  100  loss:  0.9590069651603699\n",
            "epoch:  150  loss:  0.3668096661567688\n",
            "epoch:  200  loss:  0.25554153323173523\n",
            "epoch:  250  loss:  0.015595782548189163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(3072,512),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(512,256),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(256,128),\n",
        "                      nn.Tanh(),\n",
        "                      nn.Linear(128,10),\n",
        "                      nn.LogSoftmax(dim=1))\n"
      ],
      "metadata": {
        "id": "w-MtPgFd9Lml"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for img, labels in val_loader:\n",
        "    batch_size = img.shape[0]\n",
        "    output = model(img.view(batch_size,-1))\n",
        "    _, predicted = torch.max(output,dim=1)\n",
        "    total += labels.shape[0]\n",
        "    correct+=int((predicted==labels).sum())\n",
        "print('accuracy: ', correct/total)\n"
      ],
      "metadata": {
        "id": "sZ515i2bJd0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa575ac4-faed-428a-8fdb-c512eaeb0e81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.0921\n"
          ]
        }
      ]
    }
  ]
}